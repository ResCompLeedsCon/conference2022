# list of sessions

- 
  id: 010
  title: "Registration"
  description: 
  speakers: []
  hidden: true

- 
  id: 020
  title: "Break"
  description: 
  speakers: []
  hidden: true
- 
  id: 030
  title: "Questions and changeover"
  description: 
  speakers: []
  hidden: true
- 
  id: 040
  title: "Lunch"
  description: 
  speakers: []
  hidden: true
- 
  id: 050
  title: "Social"
  description: "
  A chance for refreshments and meeting other attendees."
  speakers: []
  hidden: true
- 
  id: 060
  title: "Welcome back"
  description: 
  speakers: []
  hidden: true

# talk section
- 
  id: 102
  title: "Keynote"
  description: >
    
    To be confirmed.
    
  speakers: [0]
  hidden: false
- 
  id: 103
  title: "Invited Research Portfolio talks"
  description: 
  speakers: []
  hidden: false
- 
  id: 111
  title: "Day 1: Research Portfolio talks"
  sessions:
    - talkTitle: Digital twins of breast tumours for predicting neoadjuvant chemotherapy outcome
      startTime: 1100
      endTime: 1120
      speaker: 2
      description: >
        Glioblastoma (GBM) is the most common and deadly form of adult brain 
        cancer. GBM cancer cells are messed up! They have been molecularly 
        rewired in ways that mean they grow when they shouldn't, reshape the 
        tissue surrounding them and are currently incurable. We use large 
        scale molecular profiling to try and understand this rewiring and 
        how we can counteract it. This means we create vast amounts of data 
        from noisy samples across many different molecular levels at differing 
        resolutions (from single cell to whole tissue). We believe these data 
        hold the information we need to cure brain cancer but we to get at 
        it we have to first have to code the heck out of it!
    - talkTitle: Using HPC to predict the spread of COVID19 misinformation
      startTime: 1120
      endTime: 1140
      speaker: 1
      description: >
        I will present our project aimed to improve understanding of 
        COVID-19 misinformation chains in social media using AI tracing 
        tools.  We used our HPC cluster (1) to collect a corpus of about 
        2 million COVID-related messages and corresponding user profiles 
        from Facebook, Telegram and Twitter and (2) to develop AI classifiers 
        to make predictions about the properties of the messages and the
        profiles using Deep Learning frameworks. Our main hypothesis is 
        that the socio-demographic profile of the audience is an important 
        indicator for the likelihood of falling prey to misinformation, as 
        different readers differ in how much they might be willing to share 
        misinformation of different kinds.  We indeed found a strong positive 
        association of the likelihood of sharing COVID-19 misinformation with
        readers age and their right-wing political orientation. 
        Another association we observed concerns different preferences 
        for sharing misinformation genres, in particular, academic writing 
        vs personal stories.

    - talkTitle: "CryoEM pipelines or: How I Learned to Stop Worrying and Use all the Cores"
      startTime: 1140
      endTime: 1200
      speaker: 3
      description: >
        As Cryo-electron microscopy (cryoEM) has been transformed over the 
        last decade we face new bottlenecks in the filed. one of them been 
        real time processing of the data. New software and scripts enable us 
        to use more processing power of both CPUs and GPUs in order to deal 
        with this bottleneck. 
  hidden: false
  speakers: [2,1,3]
- 
  id: 104
  title: "15min and 10min Talks"
  description: 
  speakers: []
  hidden: false
- 
  id: 105
  title: "Lightning and poster presentations"
  description: 
  speakers: []
  hidden: false
- 
  id: 106
  title: "Closing Keynote"
  description: 
  speakers: []
  hidden: false
# panel sessions
- 
  id: 301
  title: "Discussion Panel: Open Research"
  description: |
    
    Computational methods have a part to play in helping to enable or even ensure that research is open – reproducibility. At the very minimum code and any other “processing” of data or information are an important research output especially alongside the raw and processed or generated data and perhaps even as a first class research output in their own right. The big advantage of computational or code based processing is that it is possible to rerun the analyses and verify the results. This is much more problematic with GUI based tools which rarely record the processing journey.
    Could you reproduce your own research let alone that of another researcher?
    How does this apply in the different research disciplines?

  speakers: [101]
  hidden: false
- 
  id: 302
  title: "Discussion Panel: Sustainability and research computing"
  description: |
    
    Computational communities are becoming increasingly aware of the energy cost associated with high performance computing, and the associated environmental footprint. In this panel we will discuss how to ensure that our computational research is performed responsibly. We will consider factors such as:

    1.  Making supercomputing “green”
    2.  Optimising code efficiency to minimise energy consumption
    3.  Simplifying our scientific questions to make our calculations less costly
    
  speakers: []
  hidden: false

  # code clinic and workshop sessions

- 
  id: 401
  title: "Code clinic"
  description: "
  Bring your code problems to present and share with others. A chance to collaboratively
  troubleshoot code issues. Prior submission will be necessary."
  speakers: []
  hidden: false
- 
  id: 402
  title: "Workshop"
  description: "TBC"
  speakers: []
  hidden: false

# 10min talks

- 
  id: 1801
  title: A modular, open-source pipeline for grading of Follicular Lymphoma with rapid transfer to other tasks
  description: >
    Cytological grading of Follicular Lymphoma (FL) involves identification and quantification of cancerous cells within lymph nodes. When conducted manually, it is only feasible for clinicians to sample ten sites from hundreds available. The process therefore suffers high sampling error and high inter-observer variability. Given these limitations, the utility of clinical cytological grading has been low.

    Advances in image-based deep learning have led to creation of models with expert-like performance in cancer classification. We report the development of a deep learning pipeline for automated grading of entire tissue samples, containing millions of cells. The methods are robust to technical variability, such as differences in staining; can be executed on consumer-level computing resources and can be adapted to other disease cases within a working week.

    Using the novel pipeline, we have conducted cell-level analysis of one of the largest FL image sets in the World. This has uncovered relationships between cytological grade and patient outcome. Through continued refinement, we aim to set the gold standard for cytological grading of large images, with clinical application.
  speakers: [5]
  hidden: false

- 
  id: 1802
  title: "Using ARC HPC for virtual high-throughput ligand discovery: A journey through time 2009-2022"
  description: >

    With the exponential rise in the number of viable novel drug targets, 
    computational methods are being increasingly applied to accelerate 
    the drug discovery process. Virtual High Throughput Screening 
    (vHTS) is one such established methodology to identify drug candidates 
    from large collection of compound libraries. This talk will detail 
    my journey into vHTS at Leeds from ARC1 where we were able to 
    screen tens of thousands of compounds, through to present where 
    we are screening tens of millions and hoping to go bigger. 

  speakers: [6]
  hidden: false

- 
  id: 1803
  title: "Day 2 10 minute talks session 4"
  sessions:
    - talkTitle: Digital twins of breast tumours for predicting neoadjuvant chemotherapy outcome
      startTime: 1500
      endTime: 1510
      speaker: Rose Collet
      description: |
        Breast cancer is the most common cancer in women across the globe and a major cause of death. Neoadjuvant chemotherapy (NACT) 
        is the standard of care for patients with locally advanced breast cancer, delivered to shrink the tumour before proceeding to
         surgery. However, only 39% of patients achieve pathological complete response and up to 12% experience no response at all. As
          such, there is a clear need to accurately identify non-responsive tumours as early as possible, enabling clinicians to 
          discontinue the unsuccessful NACT and proceed with alternative treatment.

        For this purpose, we propose using digital twins: virtual replicates of a patient's tumour cellularity, 
        which can be evolved in time to predict evolution under a NACT regimen. Initial tumour cellularity is 
        estimated from diffusion-weighted magnetic resonance imaging (MRI), and used to calibrate biophysically 
        relevant mathematical models of tumour growth. This calibration comes with an extremely large computational 
        expense, in particular for the coupling of the surrounding tissue stiffness to tumour cell diffusion. As a 
        first step to remedying this, we have rewritten this mechanical coupling using a finite element solver, 
        and compared code performances on synthetic datasets and MRI data from patients. 

    - talkTitle: Outcome Prediction in Pelvic Radiotherapy Using Deep Multiple Instance Learning and Cascaded Attentions.
      startTime: 1510
      endTime: 1520
      speaker: Behnaz Elhaminia
      description: |
        Radiotherapy is currently used for more than 50% of patients with various cancers. Using ionising radiation to eliminate cancerous tissue as the basis of radiotherapy can also damage normal tissues around the tumour, which leads to malfunction in those organs – called toxicity. The occurrence and severity of this toxicity vary from patient to patient and it is still not well understood which factors increase the risk of toxicity. Although deep neural networks can perform challenging tasks with high accuracy, their complexity and not being able to explain their outcome hinder their application in real-world radiotherapy tasks. In our work, we propose a novel convolutional model to predict the toxicity for patients with pelvic cancers. Our novelty is twofold; firstly, we propose to employ multiple instance learning to investigate large data including 3D CT scans and 3D dose treatment plans with lower complexity. Secondly, we apply the attention mechanism to provide visual explanation for the network's behaviour. The Quantitative and qualitative performance analysis demonstrate that our framework can offer clinically convincing tools for radiotherapy toxicity prediction. The development of investigating both image and patient numerical data with a deep network will be a very important research direction for our future work.
    - talkTitle: Integrating next generation sequencing datasets to model gene regulation during cellular differentiation
      startTime: 1520
      endTime: 1530
      speaker: Amber Emmett
      description: |
        All cells in the human body have same genetic blueprint yet have diverged to produce different proteins and perform distinct functions. This cellular diversity is achieved through regulation of gene expression, ensuring each cell switches on the right genes at the right time. As cells differentiate, non-coding regions of the genome – cis-regulatory elements – exercise tight control over gene expression through the activity of DNA-bound transcription factors. Using statistics and machine learning, we can identify transcription factor-bound cis-regulatory elements and their target genes from large, noisy next-generation sequencing datasets. Here we present an integrative ‘omics approach to identify and prioritise gene-specific cis-Regulatory Elements Across Differentiation (cisREAD). We show how sequencing datasets can be processed using high-performance computing to generate inputs to our software and demonstrate usage of the cisREAD R package.
    - talkTitle: I learned a programming language... now what?
      startTime: 1530
      endTime: 1540
      speaker: Patricia Ternes
      description: |
        Coding is no longer just a software developer activity. 
        Codes have been used for the most diverse activities across different areas. 
        In this context, people with the most diverse backgrounds have sought to learn 
        some programming language, but like any human language, there is a huge gap between 
        learning language syntax and actually engaging in a conversation about a random topic with a native speaker.<br>

        This is the scenario people are facing with coding. 
        We learn syntax, we practice with very simplified problems, 
        but when we are faced with real problems in our field, we cannot 
        even imagine how to solve them. In this talk I will present some ideas 
        to help to reduce the gap between learning a programming language and 
        being able to use it to solve problems.
    - talkTitle: "Understanding Colorectal Cancer patients' microbiome using shotgun metagenomics: a big cohort study: COLO-COHORT"
      startTime: 1540
      endTime: 1550
      speaker: Suparna Mitra
      description: |
        16000 people die of colorectal cancer (CRC) in the UK each year. More than half of CRC cases could be prevented by addressing modifiable lifestyle factors, identification of earlier stage neoplasia and targeted interventions, including chemoprevention and, crucially, polypectomy. COLO-COHORT will perform microbiome analyses on 4000 individuals undergoing colonoscopy because of symptoms or for surveillance. We will collect phenotypic information, undertake faecal immunochemical testing for occult bleeding, and obtain relevant blood investigations. It is being considered to do shotgun metagenomics. Which is a big computational challenge which include sequence assembly, alignment and annotation. After obtaining taxonomic and functional data, information will be correlated with phenotypic and the neoplasia profile at colonoscopy to identify factors which best predict disease risk. We will compare diversity and structure of the faecal microbiome in patients with and without neoplasia and correlate with dietary and/or lifestyle patterns. This project will  give valuable information on the role of the microbiome in patients with adenomas or cancer. 

        My talk will be a presentation of this research story, the pipeline planning and what problems I have faced so far. Further I will be providing few results related to healthy volunteer pilot study of this project.


    
  speakers: [12, 22, 49, 7,23]
  hidden: false

- 
  id: 1804
  title: "How do we determine the structural changes of a viral surface protein only 16 nm tall?"
  description: >

    Many viruses use their surface proteins to get inside a host cell and 
    start an infection, like coronaviruses. Here, we sought to visualize
    Bunyamwera virus (BUNV), a model for more pathogenic viruses like La 
    Crosse virus, which can cause fatal encephalitis. As viruses are small
    (~0.1 µm), we employed cryo-electron microscopy. By imaging the same
    virions from multiple orientations, we could computationally 
    reconstruct them into 3D volumes, termed tomograms. We then wanted 
    to image the surface proteins of BUNV. To do this, we utilised the 
    parallel-processing power of ARC to perform sub-tomogram averaging. 
    We selected thousands of sub-volumes from multiple viruses, 
    corresponding to the surface proteins of BUNV. These sub-volumes 
    are iteratively aligned to one another, refining their rotations 
    and locations at each stage until optimally aligned (an approach 
    termed sub-tomogram averaging). This allowed us to generate a 
    3D structure of the BUNV surface proteins, which are only ~16 nm. 
    In addition, we used Google DeepMind's AlphaFold software to predict 
    the exact amino acid structure of the BUNV surface proteins, 
    which allowed us to interpret our sub-tomogram averaging. 
    Overall, understanding the BUNV surface proteins will allow 
    the future development of vaccines or anti-virals for related 
    pathogenic viruses.
  speakers: [9]
  hidden: false
- 
  id: 1805
  title: "Big data to useful data: making use of large scale GPS data."
  description: >

    The increasing use of secondary and commercial data sources paired 
    with the decreasing cost of GPS enabled devices has seen a huge 
    increase in the amount of GPS data available to researchers. 
    These data are valuable in understanding human behaviour 
    and how the environment around us influences our health. 
    This presentation takes you thought the method developed 
    to clean individual level GPS data to the OpenStreetMap 
    Network and calculate environmental exposure(s). Scalable to 
    large-scale GPS data with global coverage this code allows 
    reproducible comparisons of environmental exposures across 
    a wide range of spatial contexts. Moreover, as the code is open 
    source and sits along existing open source packages it provides 
    a valuable tool for policy and decision makers.
    
  speakers: [10]
  hidden: false

- 
  id: 1806
  title: "Transcriptome analysis of temporal artery biopsies to identify novel pathogenic pathways based on inflammatory patterns in Giant Cell Arteritis"
  description: >

    Giant cell arteritis (GCA) is the most common form of vasculitis and can lead to serious complications, 
    such as permanent visual loss, if undiagnosed and untreated in a timely manner. The aim of my 
    PhD project is to use transcriptomic data to gain better understanding of the molecular and 
    genetic mechanisms underlying GCA and to identify candidate genes and pathways amenable to therapeutic targeting. 
    The data cohort comprises eighty patients diagnosed with GCA and includes RNA-seq data generated from temporal 
    artery biopsies, a range of clinical variables, and histological images reflecting different phenotypes relevant 
    to the pathology of GCA. The gene expression data was processed using an in-house developed pipeline of software 
    implemented on the HPC. A series of downstream analyses was performed to assess the influence of clinical variables 
    (e.g., sex, age, duration of steroids exposure) and to examine the association of transcript levels with histological
     and clinical phenotypes. The results showed that patients' sex is likely to has confounding effect and needs to be
      accounted for in the analysis. Statistical testing revealed lists of genes (statistically significant after multiple
       testing correction) associated with certain histological features. These results are currently being further 
       investigated using pathway analysis approaches.
    
  speakers: [11]
  hidden: false
- 
  id: 1807
  title: "Deconvolution of bulk transcriptomic data reveals immune cell landscape of inflammatory infiltrates in giant cell arteritis"
  description: >

    The cellular landscape of many rare diseases remains enigmatic, due to limitations of data availability. 
    Yet the knowledge of cellular composition is crucial to understand molecular events and cellular players 
    driving the conditions. Recent advances in computational deconvolution methods have made it possible to 
    infer cell type proportions from bulk RNA-seq datasets, that are more prevalent and cost-effective than 
    single-cell RNA-seq datasets. We performed deconvolution of bulk RNA-seq dataset (n=88) generated from 
    temporal artery biopsies of patients with Giant Cell Arteritis (GCA), using a single-cell RNA-seq dataset 
    (n=9,) as a reference (also generated in GCA patients). The main objective of the study was to uncover 
    cell type proportions in biopsy samples and shed light on cell-type-specific associations with clinical 
    and histological phenotypes in GCA. Several deconvolution software packages were used, and the obtained 
    cell type proportions were compared to determine methods reliability and its suitability for vascular 
    tissue data. Overall, the findings reveal a previously unreported landscape of cell population abundance 
    levels in GCA biopsies and provide novel insights into cell-type-specific expression profiles of both, 
    transcripts already known to be involved in GCA pathogenesis, as well as novel molecular signatures that 
    might have potential for therapeutic targeting. 
    
  speakers: [11]
  hidden: false



# lightning talks

- 
  id: 1001
  title: "Faster estimation of choice models using HPC"
  description: >
    The main goals of choice models are typically to predict a set of
    outcomes given a discrete set of choice alternatives and to 
    forecast choices given a change in choice context. Many 
    different economic and psychological concepts can be 
    incorporated within a choice model, leading to frequent
    implementation of very complex models.
    Additionally, recent technological advances have led to the 
    availability of a wider range of data for behavioural
    modelling. In particular, physiological data (eye-tracking, EEG, etc)
    and large-scale revealed preference data (e.g. travel card or 
    supermarket clubcard data) have meant that we have have more 
    complex and larger choice datasets.
    The combination of complex models for complex datasets requires 
    significant computational power, far beyond what can be computed
    in a reasonable time for standard desktops, 
    which may take weeks or months to compute estimates for the 
    model parameters that best explain the full set of choice observations.
    In this presentation, we demonstrate examples of how the Apollo 
    package in R allows us to run models on multiple cores, which can 
    be estimated at least 20 times faster with the Leeds HPC. 
  speakers: [4]
  hidden: false

- 
  id: 1002
  title: Medical image reconstruction under Bayesian modelling
  description: >
    Due to loss of information during the scanning process, the observed image is often blurred
    and contains noise. As a result, the observed image is generally degraded and is not helpful for
    clinical diagnostics. Bayesian methods have been identified to be particularly useful when there
    is access to limited data but with a high number of unknown parameters. Hence, our research---
    under the Bayesian hierarchical modelling structure employing multiple data sources---aims to
    develop new reconstruction methods capable of providing more robust results with increased
    image quality.
  speakers: [8]
  hidden: false
