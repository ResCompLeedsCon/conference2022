# list of sessions

- 
  id: 010
  title: "Registration"
  description: 
  speakers: []
  hidden: true

- 
  id: 020
  title: "Break"
  description: 
  speakers: []
  hidden: true
- 
  id: 030
  title: "Questions and changeover"
  description: 
  speakers: []
  hidden: true
- 
  id: 040
  title: "Lunch"
  description: 
  speakers: []
  hidden: true
- 
  id: 050
  title: "Social"
  description: "
  A chance for refreshments and meeting other attendees."
  speakers: []
  hidden: true
- 
  id: 060
  title: "Welcome back"
  description: 
  speakers: []
  hidden: true

# talk section
- 
  id: 102
  title: "Keynote"
  description: >
    
    To be confirmed.
    
  speakers: [0]
  hidden: false
- 
  id: 103
  title: "Invited Research Portfolio talks"
  description: 
  speakers: []
  hidden: false
- 
  id: 111
  title: "Curing Brain Cancer - with a Computer"
  description: >
    Glioblastoma (GBM) is the most common and deadly form of adult brain 
    cancer. GBM cancer cells are messed up! They have been molecularly 
    rewired in ways that mean they grow when they shouldn't, reshape the 
    tissue surrounding them and are currently incurable. We use large 
    scale molecular profiling to try and understand this rewiring and 
    how we can counteract it. This means we create vast amounts of data 
    from noisy samples across many different molecular levels at differing 
    resolutions (from single cell to whole tissue). We believe these data 
    hold the information we need to cure brain cancer but we to get at 
    it we have to first have to code the heck out of it!
    
  speakers: [2]
  hidden: false
- 
  id: 112
  title: "Using HPC to predict the spread of COVID19 misinformation"
  description: >
    I will present our project aimed to improve understanding of 
    COVID-19 misinformation chains in social media using AI tracing 
    tools.  We used our HPC cluster (1) to collect a corpus of about 
    2 million COVID-related messages and corresponding user profiles 
    from Facebook, Telegram and Twitter and (2) to develop AI classifiers 
    to make predictions about the properties of the messages and the
    profiles using Deep Learning frameworks. Our main hypothesis is 
    that the socio-demographic profile of the audience is an important 
    indicator for the likelihood of falling prey to misinformation, as 
    different readers differ in how much they might be willing to share 
    misinformation of different kinds.  We indeed found a strong positive 
    association of the likelihood of sharing COVID-19 misinformation with
    readers age and their right-wing political orientation. 
    Another association we observed concerns different preferences 
    for sharing misinformation genres, in particular, academic writing 
    vs personal stories.
  speakers: [1]
  hidden: false
- 
  id: 113
  title: "CryoEM pipelines or: How I Learned to Stop Worrying and Use all the Cores"
  description: >
    
    As Cryo-electron microscopy (cryoEM) has been transformed over the 
    last decade we face new bottlenecks in the filed. one of them been 
    real time processing of the data. New software and scripts enable us 
    to use more processing power of both CPUs and GPUs in order to deal 
    with this bottleneck. 
    
  speakers: [3]
  hidden: false
- 
  id: 104
  title: "15min and 10min Talks"
  description: 
  speakers: []
  hidden: false
- 
  id: 105
  title: "Lightning and poster presentations"
  description: 
  speakers: []
  hidden: false
- 
  id: 106
  title: "Closing Keynote"
  description: 
  speakers: []
  hidden: false
# panel sessions
- 
  id: 301
  title: "Discussion Panel: Open Research"
  description: |
    
    Computational methods have a part to play in helping to enable or even ensure that research is open – reproducibility. At the very minimum code and any other “processing” of data or information are an important research output especially alongside the raw and processed or generated data and perhaps even as a first class research output in their own right. The big advantage of computational or code based processing is that it is possible to rerun the analyses and verify the results. This is much more problematic with GUI based tools which rarely record the processing journey.
    Could you reproduce your own research let alone that of another researcher?
    How does this apply in the different research disciplines?

  speakers: [101]
  hidden: false
- 
  id: 302
  title: "Discussion Panel: Sustainability and research computing"
  description: |
    
    Computational communities are becoming increasingly aware of the energy cost associated with high performance computing, and the associated environmental footprint. In this panel we will discuss how to ensure that our computational research is performed responsibly. We will consider factors such as:

    1.  Making supercomputing “green”
    2.  Optimising code efficiency to minimise energy consumption
    3.  Simplifying our scientific questions to make our calculations less costly
    
  speakers: []
  hidden: false

  # code clinic and workshop sessions

- 
  id: 401
  title: "Code clinic"
  description: "
  Bring your code problems to present and share with others. A chance to collaboratively
  troubleshoot code issues. Prior submission will be necessary."
  speakers: []
  hidden: false
- 
  id: 402
  title: "Workshop"
  description: "TBC"
  speakers: []
  hidden: false

# 10min talks

- 
  id: 1801
  title: A modular, open-source pipeline for grading of Follicular Lymphoma with rapid transfer to other tasks
  description: >
    Cytological grading of Follicular Lymphoma (FL) involves identification and quantification of cancerous cells within lymph nodes. When conducted manually, it is only feasible for clinicians to sample ten sites from hundreds available. The process therefore suffers high sampling error and high inter-observer variability. Given these limitations, the utility of clinical cytological grading has been low.

    Advances in image-based deep learning have led to creation of models with expert-like performance in cancer classification. We report the development of a deep learning pipeline for automated grading of entire tissue samples, containing millions of cells. The methods are robust to technical variability, such as differences in staining; can be executed on consumer-level computing resources and can be adapted to other disease cases within a working week.

    Using the novel pipeline, we have conducted cell-level analysis of one of the largest FL image sets in the World. This has uncovered relationships between cytological grade and patient outcome. Through continued refinement, we aim to set the gold standard for cytological grading of large images, with clinical application.
  speakers: [5]
  hidden: false

- 
  id: 1802
  title: "Using ARC HPC for virtual high-throughput ligand discovery: A journey through time 2009-2022"
  description: >

    With the exponential rise in the number of viable novel drug targets, 
    computational methods are being increasingly applied to accelerate 
    the drug discovery process. Virtual High Throughput Screening 
    (vHTS) is one such established methodology to identify drug candidates 
    from large collection of compound libraries. This talk will detail 
    my journey into vHTS at Leeds from ARC1 where we were able to 
    screen tens of thousands of compounds, through to present where 
    we are screening tens of millions and hoping to go bigger. 

  speakers: [6]
  hidden: false

- 
  id: 1803
  title: "I learned a programming language... now what?"
  description: >

    Coding is no longer just a software developer activity. Codes have been used for the most diverse activities across different areas. In this context, people with the most diverse backgrounds have sought to learn some programming language, but like any human language, there is a huge gap between learning language syntax and actually engaging in a conversation about a random topic with a native speaker.

    This is the scenario people are facing with coding. We learn syntax, we practice with very simplified problems, but when we are faced with real problems in our field, we cannot even imagine how to solve them. In this talk I will present some ideas to help to reduce the gap between learning a programming language and being able to use it to solve problems.

  speakers: [7]
  hidden: false

- 
  id: 1804
  title: "How do we determine the structural changes of a viral surface protein only 16 nm tall?"
  description: >

    Many viruses use their surface proteins to get inside a host cell and 
    start an infection, like coronaviruses. Here, we sought to visualize
    Bunyamwera virus (BUNV), a model for more pathogenic viruses like La 
    Crosse virus, which can cause fatal encephalitis. As viruses are small
    (~0.1 µm), we employed cryo-electron microscopy. By imaging the same
    virions from multiple orientations, we could computationally 
    reconstruct them into 3D volumes, termed tomograms. We then wanted 
    to image the surface proteins of BUNV. To do this, we utilised the 
    parallel-processing power of ARC to perform sub-tomogram averaging. 
    We selected thousands of sub-volumes from multiple viruses, 
    corresponding to the surface proteins of BUNV. These sub-volumes 
    are iteratively aligned to one another, refining their rotations 
    and locations at each stage until optimally aligned (an approach 
    termed sub-tomogram averaging). This allowed us to generate a 
    3D structure of the BUNV surface proteins, which are only ~16 nm. 
    In addition, we used Google DeepMind's AlphaFold software to predict 
    the exact amino acid structure of the BUNV surface proteins, 
    which allowed us to interpret our sub-tomogram averaging. 
    Overall, understanding the BUNV surface proteins will allow 
    the future development of vaccines or anti-virals for related 
    pathogenic viruses.
  speakers: [9]
  hidden: false
- 
  id: 1805
  title: "Big data to useful data: making use of large scale GPS data."
  description: >

    The increasing use of secondary and commercial data sources paired 
    with the decreasing cost of GPS enabled devices has seen a huge 
    increase in the amount of GPS data available to researchers. 
    These data are valuable in understanding human behaviour 
    and how the environment around us influences our health. 
    This presentation takes you thought the method developed 
    to clean individual level GPS data to the OpenStreetMap 
    Network and calculate environmental exposure(s). Scalable to 
    large-scale GPS data with global coverage this code allows 
    reproducible comparisons of environmental exposures across 
    a wide range of spatial contexts. Moreover, as the code is open 
    source and sits along existing open source packages it provides 
    a valuable tool for policy and decision makers.
    
  speakers: [10]
  hidden: false

- 
  id: 1806
  title: "Transcriptome analysis of temporal artery biopsies to identify novel pathogenic pathways based on inflammatory patterns in Giant Cell Arteritis"
  description: >

    Giant cell arteritis (GCA) is the most common form of vasculitis and can lead to serious complications, 
    such as permanent visual loss, if undiagnosed and untreated in a timely manner. The aim of my 
    PhD project is to use transcriptomic data to gain better understanding of the molecular and 
    genetic mechanisms underlying GCA and to identify candidate genes and pathways amenable to therapeutic targeting. 
    The data cohort comprises eighty patients diagnosed with GCA and includes RNA-seq data generated from temporal 
    artery biopsies, a range of clinical variables, and histological images reflecting different phenotypes relevant 
    to the pathology of GCA. The gene expression data was processed using an in-house developed pipeline of software 
    implemented on the HPC. A series of downstream analyses was performed to assess the influence of clinical variables 
    (e.g., sex, age, duration of steroids exposure) and to examine the association of transcript levels with histological
     and clinical phenotypes. The results showed that patients' sex is likely to has confounding effect and needs to be
      accounted for in the analysis. Statistical testing revealed lists of genes (statistically significant after multiple
       testing correction) associated with certain histological features. These results are currently being further 
       investigated using pathway analysis approaches.
    
  speakers: [11]
  hidden: false
- 
  id: 1807
  title: "Deconvolution of bulk transcriptomic data reveals immune cell landscape of inflammatory infiltrates in giant cell arteritis"
  description: >

    The cellular landscape of many rare diseases remains enigmatic, due to limitations of data availability. 
    Yet the knowledge of cellular composition is crucial to understand molecular events and cellular players 
    driving the conditions. Recent advances in computational deconvolution methods have made it possible to 
    infer cell type proportions from bulk RNA-seq datasets, that are more prevalent and cost-effective than 
    single-cell RNA-seq datasets. We performed deconvolution of bulk RNA-seq dataset (n=88) generated from 
    temporal artery biopsies of patients with Giant Cell Arteritis (GCA), using a single-cell RNA-seq dataset 
    (n=9,) as a reference (also generated in GCA patients). The main objective of the study was to uncover 
    cell type proportions in biopsy samples and shed light on cell-type-specific associations with clinical 
    and histological phenotypes in GCA. Several deconvolution software packages were used, and the obtained 
    cell type proportions were compared to determine methods reliability and its suitability for vascular 
    tissue data. Overall, the findings reveal a previously unreported landscape of cell population abundance 
    levels in GCA biopsies and provide novel insights into cell-type-specific expression profiles of both, 
    transcripts already known to be involved in GCA pathogenesis, as well as novel molecular signatures that 
    might have potential for therapeutic targeting. 
    
  speakers: [11]
  hidden: false



# lightning talks

- 
  id: 1001
  title: "Faster estimation of choice models using HPC"
  description: >
    The main goals of choice models are typically to predict a set of
    outcomes given a discrete set of choice alternatives and to 
    forecast choices given a change in choice context. Many 
    different economic and psychological concepts can be 
    incorporated within a choice model, leading to frequent
    implementation of very complex models.
    Additionally, recent technological advances have led to the 
    availability of a wider range of data for behavioural
    modelling. In particular, physiological data (eye-tracking, EEG, etc)
    and large-scale revealed preference data (e.g. travel card or 
    supermarket clubcard data) have meant that we have have more 
    complex and larger choice datasets.
    The combination of complex models for complex datasets requires 
    significant computational power, far beyond what can be computed
    in a reasonable time for standard desktops, 
    which may take weeks or months to compute estimates for the 
    model parameters that best explain the full set of choice observations.
    In this presentation, we demonstrate examples of how the Apollo 
    package in R allows us to run models on multiple cores, which can 
    be estimated at least 20 times faster with the Leeds HPC. 
  speakers: [4]
  hidden: false

- 
  id: 1002
  title: Medical image reconstruction under Bayesian modelling
  description: >
    Due to loss of information during the scanning process, the observed image is often blurred
    and contains noise. As a result, the observed image is generally degraded and is not helpful for
    clinical diagnostics. Bayesian methods have been identified to be particularly useful when there
    is access to limited data but with a high number of unknown parameters. Hence, our research---
    under the Bayesian hierarchical modelling structure employing multiple data sources---aims to
    develop new reconstruction methods capable of providing more robust results with increased
    image quality.
  speakers: [8]
  hidden: false
